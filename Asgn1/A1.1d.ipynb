{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e42bee-8301-4473-8e56-15bc7fb55d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b74058ed-954a-499e-bd0a-55cfcea52308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data from mnist\n",
    "num_classes = 10\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.tensor(y,dtype=torch.float))\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.tensor(y,dtype=torch.float))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78f603c9-ae4b-45ab-80f6-9649152245a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape [NCHW]: torch.Size([1024, 1, 28, 28])\n",
      "y shape: torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "#create the dataloaders: \n",
    "batch_size = 1024\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X,y in train_dataloader:\n",
    "    print(f\"X shape [NCHW]: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32ffc71a-b93d-4f53-aadb-a1aa006aa038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "#define the device and the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "alpha=4.5\n",
    "beta=4.5\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1,8,5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    nn.Conv2d(8,16,5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(256,128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128,32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32,1),\n",
    ")\n",
    "\n",
    "output_tf = lambda mx: alpha*mx + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab99fd9e-49e9-43f0-9ff5-3f0fe7765e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimiazer, loss and training\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies =[]\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    \n",
    "    losses,accuracies = [],[]\n",
    "    for lvb, (X,y) in enumerate(dataloader):\n",
    "        X.to(device), y.to(device)\n",
    "\n",
    "        #forward pass\n",
    "        mx = model(X)\n",
    "        mx = mx.squeeze()\n",
    "        mx = output_tf(mx)\n",
    "        loss = loss_fn(mx,y)\n",
    "\n",
    "        #gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # append loss and accuracy for recording\n",
    "        losses.append(loss.item())\n",
    "        acc = (y == torch.round(mx)).mean(dtype = float).item()\n",
    "        accuracies.append(acc)\n",
    "\n",
    "        #print for following\n",
    "        if lvb % 10 == 0:\n",
    "            loss, current = loss.item(), lvb * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    train_loss = np.mean(losses)\n",
    "    train_acc = np.mean(accuracies)\n",
    "    \n",
    "    return train_loss, train_acc\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96a7f8e2-f66d-4e75-9a7a-895c1f70a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_loss,correct = 0,0\n",
    "        for lvb, (X,y) in enumerate(dataloader):\n",
    "            X.to(device), y.to(device)\n",
    "            \n",
    "            #forward pass\n",
    "            mx = model(X)\n",
    "            mx = mx.squeeze()\n",
    "            mx = output_tf(mx)\n",
    "            loss = loss_fn(mx,y)\n",
    "            \n",
    "            # append loss and accuracy for recording\n",
    "            test_loss += loss.item()\n",
    "            correct += (y == torch.round(mx)).sum().item()\n",
    "                \n",
    "    test_loss /= num_batches\n",
    "    acc = correct / size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*acc):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88149ece-5f34-4e1f-9021-4df381e2f2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE-TRAINING:\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 9.7%, Avg loss: 8.335621 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 8.307939  [    0/60000]\n",
      "loss: 7.552577  [10240/60000]\n",
      "loss: 5.966996  [20480/60000]\n",
      "loss: 4.125992  [30720/60000]\n",
      "loss: 3.689331  [40960/60000]\n",
      "loss: 3.047615  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 25.4%, Avg loss: 3.023489 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.036360  [    0/60000]\n",
      "loss: 2.728499  [10240/60000]\n",
      "loss: 2.564800  [20480/60000]\n",
      "loss: 2.585215  [30720/60000]\n",
      "loss: 2.332980  [40960/60000]\n",
      "loss: 1.874583  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.7%, Avg loss: 1.866092 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.864504  [    0/60000]\n",
      "loss: 1.724776  [10240/60000]\n",
      "loss: 1.972403  [20480/60000]\n",
      "loss: 1.863144  [30720/60000]\n",
      "loss: 1.815627  [40960/60000]\n",
      "loss: 1.392230  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 1.418383 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.409383  [    0/60000]\n",
      "loss: 1.334795  [10240/60000]\n",
      "loss: 1.639266  [20480/60000]\n",
      "loss: 1.450125  [30720/60000]\n",
      "loss: 1.465590  [40960/60000]\n",
      "loss: 1.077703  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1.129846 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.111066  [    0/60000]\n",
      "loss: 1.092834  [10240/60000]\n",
      "loss: 1.345594  [20480/60000]\n",
      "loss: 1.169916  [30720/60000]\n",
      "loss: 1.231429  [40960/60000]\n",
      "loss: 0.883256  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 0.957665 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.942874  [    0/60000]\n",
      "loss: 0.921456  [10240/60000]\n",
      "loss: 1.145405  [20480/60000]\n",
      "loss: 0.962171  [30720/60000]\n",
      "loss: 1.039801  [40960/60000]\n",
      "loss: 0.750087  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.849349 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.841378  [    0/60000]\n",
      "loss: 0.803052  [10240/60000]\n",
      "loss: 1.014816  [20480/60000]\n",
      "loss: 0.834449  [30720/60000]\n",
      "loss: 0.904524  [40960/60000]\n",
      "loss: 0.662117  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 0.769681 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.762558  [    0/60000]\n",
      "loss: 0.702182  [10240/60000]\n",
      "loss: 0.907540  [20480/60000]\n",
      "loss: 0.732446  [30720/60000]\n",
      "loss: 0.800593  [40960/60000]\n",
      "loss: 0.600813  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.703458 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.699729  [    0/60000]\n",
      "loss: 0.622535  [10240/60000]\n",
      "loss: 0.822861  [20480/60000]\n",
      "loss: 0.663235  [30720/60000]\n",
      "loss: 0.730668  [40960/60000]\n",
      "loss: 0.557514  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.653855 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.652340  [    0/60000]\n",
      "loss: 0.561026  [10240/60000]\n",
      "loss: 0.750819  [20480/60000]\n",
      "loss: 0.610938  [30720/60000]\n",
      "loss: 0.677502  [40960/60000]\n",
      "loss: 0.525610  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.608591 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.610651  [    0/60000]\n",
      "loss: 0.514694  [10240/60000]\n",
      "loss: 0.689894  [20480/60000]\n",
      "loss: 0.569291  [30720/60000]\n",
      "loss: 0.633059  [40960/60000]\n",
      "loss: 0.496659  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.570875 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.577255  [    0/60000]\n",
      "loss: 0.476030  [10240/60000]\n",
      "loss: 0.640045  [20480/60000]\n",
      "loss: 0.533727  [30720/60000]\n",
      "loss: 0.593739  [40960/60000]\n",
      "loss: 0.472181  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.538533 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.549932  [    0/60000]\n",
      "loss: 0.444356  [10240/60000]\n",
      "loss: 0.597439  [20480/60000]\n",
      "loss: 0.505200  [30720/60000]\n",
      "loss: 0.559294  [40960/60000]\n",
      "loss: 0.451278  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.508613 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.526084  [    0/60000]\n",
      "loss: 0.416203  [10240/60000]\n",
      "loss: 0.560632  [20480/60000]\n",
      "loss: 0.478230  [30720/60000]\n",
      "loss: 0.528254  [40960/60000]\n",
      "loss: 0.431791  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.481409 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.504354  [    0/60000]\n",
      "loss: 0.390725  [10240/60000]\n",
      "loss: 0.531913  [20480/60000]\n",
      "loss: 0.454496  [30720/60000]\n",
      "loss: 0.500773  [40960/60000]\n",
      "loss: 0.410620  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.457728 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.485059  [    0/60000]\n",
      "loss: 0.368009  [10240/60000]\n",
      "loss: 0.505831  [20480/60000]\n",
      "loss: 0.432644  [30720/60000]\n",
      "loss: 0.477301  [40960/60000]\n",
      "loss: 0.393012  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.436641 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.468246  [    0/60000]\n",
      "loss: 0.347166  [10240/60000]\n",
      "loss: 0.481433  [20480/60000]\n",
      "loss: 0.410569  [30720/60000]\n",
      "loss: 0.455926  [40960/60000]\n",
      "loss: 0.375026  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.415954 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.451174  [    0/60000]\n",
      "loss: 0.330683  [10240/60000]\n",
      "loss: 0.457353  [20480/60000]\n",
      "loss: 0.389924  [30720/60000]\n",
      "loss: 0.437579  [40960/60000]\n",
      "loss: 0.359252  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.396105 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.435976  [    0/60000]\n",
      "loss: 0.314130  [10240/60000]\n",
      "loss: 0.436269  [20480/60000]\n",
      "loss: 0.371037  [30720/60000]\n",
      "loss: 0.422534  [40960/60000]\n",
      "loss: 0.343938  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.378049 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.422111  [    0/60000]\n",
      "loss: 0.300994  [10240/60000]\n",
      "loss: 0.415295  [20480/60000]\n",
      "loss: 0.355985  [30720/60000]\n",
      "loss: 0.406722  [40960/60000]\n",
      "loss: 0.330986  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.362632 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.410261  [    0/60000]\n",
      "loss: 0.289232  [10240/60000]\n",
      "loss: 0.396710  [20480/60000]\n",
      "loss: 0.342371  [30720/60000]\n",
      "loss: 0.391978  [40960/60000]\n",
      "loss: 0.317201  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.348256 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.400623  [    0/60000]\n",
      "loss: 0.276855  [10240/60000]\n",
      "loss: 0.380820  [20480/60000]\n",
      "loss: 0.330755  [30720/60000]\n",
      "loss: 0.380982  [40960/60000]\n",
      "loss: 0.304481  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.333999 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.389771  [    0/60000]\n",
      "loss: 0.265446  [10240/60000]\n",
      "loss: 0.365414  [20480/60000]\n",
      "loss: 0.319078  [30720/60000]\n",
      "loss: 0.369025  [40960/60000]\n",
      "loss: 0.294615  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.322174 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.382239  [    0/60000]\n",
      "loss: 0.256180  [10240/60000]\n",
      "loss: 0.352826  [20480/60000]\n",
      "loss: 0.308885  [30720/60000]\n",
      "loss: 0.357717  [40960/60000]\n",
      "loss: 0.283527  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.311417 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.373810  [    0/60000]\n",
      "loss: 0.250137  [10240/60000]\n",
      "loss: 0.341326  [20480/60000]\n",
      "loss: 0.298177  [30720/60000]\n",
      "loss: 0.345975  [40960/60000]\n",
      "loss: 0.275662  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.300593 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.366408  [    0/60000]\n",
      "loss: 0.240890  [10240/60000]\n",
      "loss: 0.330431  [20480/60000]\n",
      "loss: 0.290938  [30720/60000]\n",
      "loss: 0.336032  [40960/60000]\n",
      "loss: 0.267522  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.289136 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.357916  [    0/60000]\n",
      "loss: 0.232930  [10240/60000]\n",
      "loss: 0.320275  [20480/60000]\n",
      "loss: 0.280786  [30720/60000]\n",
      "loss: 0.328394  [40960/60000]\n",
      "loss: 0.258777  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.277503 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.348147  [    0/60000]\n",
      "loss: 0.226082  [10240/60000]\n",
      "loss: 0.311555  [20480/60000]\n",
      "loss: 0.272260  [30720/60000]\n",
      "loss: 0.319062  [40960/60000]\n",
      "loss: 0.251445  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.267614 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.339120  [    0/60000]\n",
      "loss: 0.218732  [10240/60000]\n",
      "loss: 0.300754  [20480/60000]\n",
      "loss: 0.265488  [30720/60000]\n",
      "loss: 0.310824  [40960/60000]\n",
      "loss: 0.243370  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.258694 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.331565  [    0/60000]\n",
      "loss: 0.213886  [10240/60000]\n",
      "loss: 0.290583  [20480/60000]\n",
      "loss: 0.258177  [30720/60000]\n",
      "loss: 0.303897  [40960/60000]\n",
      "loss: 0.237424  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.250528 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.323432  [    0/60000]\n",
      "loss: 0.207816  [10240/60000]\n",
      "loss: 0.282788  [20480/60000]\n",
      "loss: 0.252551  [30720/60000]\n",
      "loss: 0.296017  [40960/60000]\n",
      "loss: 0.230121  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.241678 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.315094  [    0/60000]\n",
      "loss: 0.203334  [10240/60000]\n",
      "loss: 0.274651  [20480/60000]\n",
      "loss: 0.246479  [30720/60000]\n",
      "loss: 0.288162  [40960/60000]\n",
      "loss: 0.224548  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.233732 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.306626  [    0/60000]\n",
      "loss: 0.197077  [10240/60000]\n",
      "loss: 0.267301  [20480/60000]\n",
      "loss: 0.240607  [30720/60000]\n",
      "loss: 0.281373  [40960/60000]\n",
      "loss: 0.217851  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.225606 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.298967  [    0/60000]\n",
      "loss: 0.191272  [10240/60000]\n",
      "loss: 0.258164  [20480/60000]\n",
      "loss: 0.234675  [30720/60000]\n",
      "loss: 0.275239  [40960/60000]\n",
      "loss: 0.211768  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.219298 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.291745  [    0/60000]\n",
      "loss: 0.186671  [10240/60000]\n",
      "loss: 0.251133  [20480/60000]\n",
      "loss: 0.231099  [30720/60000]\n",
      "loss: 0.270146  [40960/60000]\n",
      "loss: 0.206135  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.212439 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.284320  [    0/60000]\n",
      "loss: 0.181943  [10240/60000]\n",
      "loss: 0.242107  [20480/60000]\n",
      "loss: 0.225977  [30720/60000]\n",
      "loss: 0.264050  [40960/60000]\n",
      "loss: 0.200819  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.206727 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.277777  [    0/60000]\n",
      "loss: 0.178937  [10240/60000]\n",
      "loss: 0.235871  [20480/60000]\n",
      "loss: 0.221334  [30720/60000]\n",
      "loss: 0.260902  [40960/60000]\n",
      "loss: 0.195137  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.201142 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.271410  [    0/60000]\n",
      "loss: 0.175060  [10240/60000]\n",
      "loss: 0.229238  [20480/60000]\n",
      "loss: 0.215804  [30720/60000]\n",
      "loss: 0.253875  [40960/60000]\n",
      "loss: 0.191474  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.196409 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.266792  [    0/60000]\n",
      "loss: 0.171007  [10240/60000]\n",
      "loss: 0.222291  [20480/60000]\n",
      "loss: 0.212051  [30720/60000]\n",
      "loss: 0.249330  [40960/60000]\n",
      "loss: 0.186027  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.191337 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.260487  [    0/60000]\n",
      "loss: 0.167606  [10240/60000]\n",
      "loss: 0.216911  [20480/60000]\n",
      "loss: 0.206300  [30720/60000]\n",
      "loss: 0.243988  [40960/60000]\n",
      "loss: 0.181036  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.185974 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.252841  [    0/60000]\n",
      "loss: 0.163790  [10240/60000]\n",
      "loss: 0.211142  [20480/60000]\n",
      "loss: 0.201595  [30720/60000]\n",
      "loss: 0.238497  [40960/60000]\n",
      "loss: 0.175663  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.180574 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.245450  [    0/60000]\n",
      "loss: 0.160587  [10240/60000]\n",
      "loss: 0.204154  [20480/60000]\n",
      "loss: 0.196987  [30720/60000]\n",
      "loss: 0.233218  [40960/60000]\n",
      "loss: 0.172322  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.176428 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.240610  [    0/60000]\n",
      "loss: 0.156608  [10240/60000]\n",
      "loss: 0.197556  [20480/60000]\n",
      "loss: 0.190988  [30720/60000]\n",
      "loss: 0.228048  [40960/60000]\n",
      "loss: 0.168482  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.171594 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.234633  [    0/60000]\n",
      "loss: 0.152828  [10240/60000]\n",
      "loss: 0.192531  [20480/60000]\n",
      "loss: 0.184610  [30720/60000]\n",
      "loss: 0.222842  [40960/60000]\n",
      "loss: 0.164751  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.166489 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.227149  [    0/60000]\n",
      "loss: 0.150153  [10240/60000]\n",
      "loss: 0.187640  [20480/60000]\n",
      "loss: 0.180203  [30720/60000]\n",
      "loss: 0.217911  [40960/60000]\n",
      "loss: 0.161361  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.3%, Avg loss: 0.163068 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.222475  [    0/60000]\n",
      "loss: 0.148121  [10240/60000]\n",
      "loss: 0.183828  [20480/60000]\n",
      "loss: 0.176659  [30720/60000]\n",
      "loss: 0.212316  [40960/60000]\n",
      "loss: 0.157465  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.158403 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.217113  [    0/60000]\n",
      "loss: 0.145463  [10240/60000]\n",
      "loss: 0.178081  [20480/60000]\n",
      "loss: 0.172128  [30720/60000]\n",
      "loss: 0.206865  [40960/60000]\n",
      "loss: 0.154255  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.155093 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.211765  [    0/60000]\n",
      "loss: 0.142642  [10240/60000]\n",
      "loss: 0.172332  [20480/60000]\n",
      "loss: 0.167882  [30720/60000]\n",
      "loss: 0.202098  [40960/60000]\n",
      "loss: 0.150436  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.151859 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.207492  [    0/60000]\n",
      "loss: 0.139034  [10240/60000]\n",
      "loss: 0.166806  [20480/60000]\n",
      "loss: 0.163260  [30720/60000]\n",
      "loss: 0.196710  [40960/60000]\n",
      "loss: 0.146310  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.147798 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.202258  [    0/60000]\n",
      "loss: 0.136094  [10240/60000]\n",
      "loss: 0.162119  [20480/60000]\n",
      "loss: 0.159411  [30720/60000]\n",
      "loss: 0.191879  [40960/60000]\n",
      "loss: 0.143338  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.144936 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train:\n",
    "num_epochs = 50\n",
    "train_losses,train_accs, test_losses, test_accs = [],[],[],[]\n",
    "\n",
    "print(f\"PRE-TRAINING:\\n-------------------------------\")\n",
    "#test + record\n",
    "test_loss,test_acc = test(test_dataloader,model,loss_fn)\n",
    "test_losses.append(test_loss)\n",
    "test_accs.append(test_acc)\n",
    "\n",
    "for lve in range(num_epochs):\n",
    "    print(f\"Epoch {lve+1}\\n-------------------------------\")\n",
    "    #train + record\n",
    "    train_loss, train_acc = train(train_dataloader,model,loss_fn,optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    #test + record\n",
    "    test_loss,test_acc = test(test_dataloader,model,loss_fn)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d93119e-c6f4-41d9-95af-afc960ab33c5",
   "metadata": {},
   "source": [
    "Here we can see the training occurs a good deal faster than in the same case prior to the affine transformation - why?  My first hypothesis was that we wanted to scale the gradients to make it go faster, but I think Adam's adaptive optimisation should cancel this out anyways; now I'm fairly sure that the idea is to effectively re-scale the y-values (since it's functionally the same to do the inverse transformation on y, allowing that the specific form of the loss will change) -- this just moves the 'average' label to be zero instead of 4.5.  I would guess that this makes things easier just because the random initial outputs are distributed in some way around zero, which means gradients for getting different numbers wrong by the same amount have different magnitudes of loss - an error classifying 9 might hurt much more than the loss from misclassifying 0, and so presumably it trains much faster on 9s than on 0s, and this difference makes it take a lot longer overall.  I think it's fairly clear that any architecture which assumes \"1\" is closer to \"2\" than to \"3\" is working against the natural structure of the problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
